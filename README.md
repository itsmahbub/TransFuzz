# TransFuzz

## Artifact Overview

This repository contains all artifacts required to reproduce and verify the results reported in the TransFuzz paper. The artifacts and their corresponding locations are summarized below.

- **TransFuzz Framework Implementation**  
  Core implementation and command-line interface of TransFuzz.  
  *Paths:* `fuzzer.py`, `transfuzz.py`

- **Target Models and Seed Inputs**  
  Model wrappers and loading logic for the vision and speech models used in the evaluation.  
  The model wrappers automatically fetch publicly available pretrained models and seed inputs from standard benchmark datasets.  
  ImageNet seed can be downloaded using the `download_imagenet.py` script.  
  Targeted adversarial testing (RQ3) uses fine-tuned model checkpoints (`mobilevit_unsafebench` and `resnet50_unsafebench.pth`) that are included in the artifact.  
  *Path:* `model_wrappers/`  
  `**` Some pretrained models and datasets used by TransFuzz are retrieved from public hosting platforms and require standard user authentication (see **External Dataset and Model Access** )  

- **Experiment Scripts**  
  Scripts to reproduce all experiments reported in the paper.  
  *Paths:* `run_exp_*.sh`

- **Evaluation and Analysis Scripts**  
  Scripts for computing evaluation metrics (coverage, fault discovery, perceptual naturalness, fault stability, and cross-model transferability), aggregating results, and generating the tables and figures reported in the paper.  
  In these scripts, the label `yuan` refers to the baseline method proposed by Yuan et al., which is used for comparison in the paper.
  *Path:* `analysis/`

- **Precomputed Experiment Results**  
  Precomputed results for all fuzzing configurations reported in the paper and TransFuzz runtime statistics.
  In the results file, the label `yuan` refers to the baseline method proposed by Yuan et al., which is used for comparison in the paper.  
  *Paths:* `results.json`, `initial_coverage.json`, `outputs/`

- **Representative Fault-Inducing Examples**  
  A small set of representative fault-inducing inputs generated by TransFuzz for qualitative inspection.  
  *Path:* `outputs/` (subset)


### External Dataset and Model Access

Some pretrained models and datasets used by TransFuzz are retrieved from public hosting platforms and require standard user authentication:

- **Hugging Face**:  
  Several pretrained models require a free Hugging Face account and access token.  
  Authentication follows Hugging Face's standard procedure:
  https://huggingface.co/docs/hub/security-tokens

- **Kaggle**:  
  ImageNet seed input download script (`download_imagenet.py`) require a free Kaggle account and API token.  Instructions for setting up Kaggle API access are available at: https://www.kaggle.com/docs/api

These authentication requirements are standard for the respective platforms, require no special approvals, and do not restrict access for artifact evaluation.


## Target Models and Seed Inputs

TransFuzz automatically retrieves all publicly available pretrained models and seed datasets used in the paper.

### Evaluated Models
- ResNet-50 (ImageNet)
- MobileViT (ImageNet)
- ResNet-50 (UnsafeBench)
- MobileViT (UnsafeBench)
- AST (Speech Commands)
- Wav2Vec2 (Speech Commands)

## Environment Setup

The experiments were conducted using Python 3.10 with PyTorch installed with CUDA support (tested with CUDA 12.1).

TransFuzz requires Python 3 and a CUDA-enabled GPU environment. The provided `requirements.txt` installs PyTorch with CUDA support. The experiments were conducted using Python 3.10 with PyTorch installed with CUDA support (with CUDA 12.1).

```bash
python3 -m venv transfuzz-env
source transfuzz-env/bin/activate
pip install -r requirements.txt
```

## TransFuzz Interface

`transfuzz.py` exposes the following command-line arguments to control the fuzzing process:

| Argument | Description |
|--------|-------------|
| `--model` | Target model to fuzz (required) |
| `--seed-dataset` | Dataset to use for seeds |
| `--seed-count`   | Number of seed inputs from the dataset to use (-1 for all) |
| `--split`  | Dataset split to use for seed inputs |
| `--coverage-metric` | Coverage signal used for feedback (default: NLC) |
| `--target-label` | Enables targeted adversarial testing |
| `--random-mutation` | Disables gradient-guided mutation |
| `--time-budget` | Maximum fuzzing time in seconds |
| `--N` | Number of seed inputs associated with a single perturbation |
| `--seed` | Random seed for reproducibility |


## Quick Start

### Fuzz pretrained ResNet-50 image classification model

```bash
#  Download ImageNet seed inputs. This will store the seed inputs in `seeds` directory
python download_imagenet.py

# Run `TransFuzz`
python transfuzz.py --model resnet50 --seed-dataset ImageNet  --split val --time-budget 300 --N 24
```

### Fuzz pretrained AST keyword spotting model with (target label = 24, "off")

```bash
python transfuzz.py --model mitast --seed-dataset speech_commands  --split test --time-budget 300 --N 24 --target-label 24
```

## Reproducing Paper Experiments

We provide scripts to reproduce all experiments reported in the paper.

### RQ1: Adversarial Fault Discovery and Coverage
```bash
bash scripts/run_rq1.sh
```

## Precomputed Experiment Results

Each entry in `results.json` corresponds to a single fuzzing configuration. The key encodes the experimental setup (model, target label, dataset, mutation strategy, number of samples associated with a perturbation (N), and random seed). Format of the key is ` f"{model_name}-{model_path}-{dataset_name}-{target_label}-{N}-{random_seed}{rand}"`. For example:

- `resnet50-None-ImageNet-None-24-0` denotes fuzzing ResNet-50 on ImageNet with untargeted testing, N= 24, and random seed 0.

Each entry contains the following fields:

- **Configuration Metadata**  
  - `seed_count`: number of seed inputs used  
  - `clean_seed`: number of correctly classified seed inputs  
  - `time_budget`: fuzzing time budget (seconds)  
  - `N`: number of inputs associated with a single perturbation  
  - `dataset`: seed dataset  
  - `target_label`: target class for targeted testing (null for untargeted)  
  - `number_of_classes`: number of output classes  

- **Diversity** (`diversity`)  
  - `count`: number of fault-inducing inputs generated  
  - `class_covered`:  mumber of distinct output classes reached by discovered faults 
  - `scaled_entropy`: normalized Shannon entropy of the adversarial label distribution

- **Perceptual Naturalness Metrics** (`naturalness`)  
  Includes mean LPIPS, SSIM for image inputs and PESQ, STOI for audio inputs over generated fault-inducing inputs.

- **Cross-Model Transferability** (`model_transfer-*`)  
  Transfer success statistics when adversarial inputs generated on one model are evaluated on another model.

- **Coverage Improvement** (`coverage`)  
  Coverage achieved by the union of seed inputs and adversarial inputs (`new`) and only adversarial inputs (`adv`).

- **Fault Stability Metrics** (`stability`)  
  Statistics measuring whether adversarial behaviors persist under standard I/O operations.

The analysis scripts aggregate these entries across configurations to generate all tables and figures reported in the paper.


## Reproducing Tables and Figures

Scripts in `analysis/` aggregate experimental outputs and generate the tables and figures reported in the paper.

### Table Rows

Each script corresponds to a specific table in the paper.  
Within each script, the fuzzing configurations to be aggregated (e.g., model, seed dataset, N, and target label, mutation gradient direction enabled/disabled) are specified explicitly as configuration variables at the top of the file.


```bash
python analysis/rq1_row.py (Table 1)
python analysis/rq2a_row.py (Table 2a)
python analysis/rq2b_row.py (Table 2b)
python analysis/rq3a_row.py (Table 3a)
python analysis/rq3b_row.py (Table 3b)
python analysis/rq4_row.py (Table 4)
```

### Figures

The following scripts generate the figures reported in the paper using the TransFuzz experiment outputs:

- `fault_counts_over_time.py` generates `Figure 2` by aggregating runtime statistics recorded during fuzzing (stored in the outputs/ directory).

- `plot_images_best_worst_v2.py` generates Figure 3 by analyzing all generated fault-inducing inputs and ranking them using perceptual similarity metrics (LPIPS). The script visualizes the most and least perceptually natural adversarial faults discovered by TransFuzz and the Yuan et al. baseline.

## Representative Fault-Inducing Examples

The `examples/representative_faults/` directory contains a small subset of fault-inducing inputs generated by TransFuzz for qualitative inspection.

These examples are provided for illustration only and do not include the full set of generated adversarial inputs.
